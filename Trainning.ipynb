{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "intents = json.loads(open('intents.json').read())\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?', '!', '.', ',']\n",
    "\n",
    "# Process intents and patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Lemmatize and clean words\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "# Save words and classes\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get synonyms\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word and lemma.name() not in synonyms:\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms[:2]  # Return up to 2 synonyms\n",
    "\n",
    "# Augment training data\n",
    "augmented_documents = documents.copy()\n",
    "for doc, tag in documents:\n",
    "    augmented_pattern = []\n",
    "    for word in doc:\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            # Add a pattern with one word replaced by its synonym\n",
    "            for syn in synonyms:\n",
    "                new_pattern = doc.copy()\n",
    "                new_pattern[new_pattern.index(word)] = syn\n",
    "                augmented_documents.append((new_pattern, tag))\n",
    "\n",
    "documents = augmented_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    \n",
    "    # Create bag of words\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append(bag + output_row)\n",
    "\n",
    "# Shuffle and convert to numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# Split features and labels\n",
    "X = training[:, :len(words)]\n",
    "y = training[:, len(words):]\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with improved architecture\n",
    "model = tf.keras.Sequential([\n",
    "    # Input layer with more units and batch normalization\n",
    "    tf.keras.layers.Dense(512, input_shape=(len(words),), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    # Hidden layers with decreasing units\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(len(classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with modified parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Monitor accuracy instead of loss\n",
    "    patience=30,            # Increased patience\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.001        # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Add learning rate reduction\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=10,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Get the true labels (convert one-hot encoded y_train to class indices)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "\n",
    "# Convert to dictionary\n",
    "class_weight_dict = dict(zip(np.unique(y_train_labels), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "340/340 [==============================] - 4s 6ms/step - loss: 3.4786 - accuracy: 0.3901 - val_loss: 3.0626 - val_accuracy: 0.7351 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.8942 - accuracy: 0.8633 - val_loss: 0.3264 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.4037 - accuracy: 0.9268 - val_loss: 0.2144 - val_accuracy: 0.9352 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.3139 - accuracy: 0.9280 - val_loss: 0.2163 - val_accuracy: 0.9338 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.2721 - accuracy: 0.9367 - val_loss: 0.2070 - val_accuracy: 0.9367 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.2703 - accuracy: 0.9374 - val_loss: 0.2110 - val_accuracy: 0.9433 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.2462 - accuracy: 0.9384 - val_loss: 0.2261 - val_accuracy: 0.9360 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.2323 - accuracy: 0.9413 - val_loss: 0.2092 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.2276 - accuracy: 0.9433 - val_loss: 0.2260 - val_accuracy: 0.9426 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.2183 - accuracy: 0.9415 - val_loss: 0.2479 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.2205 - accuracy: 0.9431 - val_loss: 0.2513 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.2128 - accuracy: 0.9420 - val_loss: 0.2260 - val_accuracy: 0.9389 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.2006 - accuracy: 0.9430 - val_loss: 0.2503 - val_accuracy: 0.9367 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.2075 - accuracy: 0.9430 - val_loss: 0.2187 - val_accuracy: 0.9411 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1996 - accuracy: 0.9442 - val_loss: 0.2476 - val_accuracy: 0.9330 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.1476 - accuracy: 0.9564 - val_loss: 0.2043 - val_accuracy: 0.9382 - lr: 2.0000e-04\n",
      "Epoch 17/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1418 - accuracy: 0.9544 - val_loss: 0.2043 - val_accuracy: 0.9375 - lr: 2.0000e-04\n",
      "Epoch 18/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1289 - accuracy: 0.9566 - val_loss: 0.2063 - val_accuracy: 0.9389 - lr: 2.0000e-04\n",
      "Epoch 19/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1270 - accuracy: 0.9568 - val_loss: 0.2047 - val_accuracy: 0.9367 - lr: 2.0000e-04\n",
      "Epoch 20/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.1279 - accuracy: 0.9573 - val_loss: 0.2028 - val_accuracy: 0.9404 - lr: 2.0000e-04\n",
      "Epoch 21/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.1269 - accuracy: 0.9564 - val_loss: 0.2017 - val_accuracy: 0.9404 - lr: 2.0000e-04\n",
      "Epoch 22/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.9558 - val_loss: 0.2070 - val_accuracy: 0.9382 - lr: 2.0000e-04\n",
      "Epoch 23/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1277 - accuracy: 0.9556 - val_loss: 0.2064 - val_accuracy: 0.9367 - lr: 2.0000e-04\n",
      "Epoch 24/500\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 0.1288 - accuracy: 0.9555 - val_loss: 0.2089 - val_accuracy: 0.9382 - lr: 2.0000e-04\n",
      "Epoch 25/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1227 - accuracy: 0.9577 - val_loss: 0.2170 - val_accuracy: 0.9367 - lr: 2.0000e-04\n",
      "Epoch 26/500\n",
      "340/340 [==============================] - 3s 8ms/step - loss: 0.1185 - accuracy: 0.9586 - val_loss: 0.2170 - val_accuracy: 0.9367 - lr: 2.0000e-04\n",
      "Epoch 27/500\n",
      "340/340 [==============================] - 2s 7ms/step - loss: 0.1292 - accuracy: 0.9558 - val_loss: 0.2119 - val_accuracy: 0.9389 - lr: 2.0000e-04\n",
      "Epoch 28/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1263 - accuracy: 0.9542 - val_loss: 0.2182 - val_accuracy: 0.9360 - lr: 2.0000e-04\n",
      "Epoch 29/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1227 - accuracy: 0.9580 - val_loss: 0.2168 - val_accuracy: 0.9389 - lr: 2.0000e-04\n",
      "Epoch 30/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1219 - accuracy: 0.9566 - val_loss: 0.2154 - val_accuracy: 0.9389 - lr: 2.0000e-04\n",
      "Epoch 31/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1171 - accuracy: 0.9564 - val_loss: 0.2198 - val_accuracy: 0.9375 - lr: 2.0000e-04\n",
      "Epoch 32/500\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 0.1173 - accuracy: 0.9586 - val_loss: 0.2190 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
      "Epoch 33/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1080 - accuracy: 0.9591 - val_loss: 0.2199 - val_accuracy: 0.9375 - lr: 1.0000e-04\n",
      "Epoch 34/500\n",
      "340/340 [==============================] - 2s 5ms/step - loss: 0.1115 - accuracy: 0.9584 - val_loss: 0.2164 - val_accuracy: 0.9345 - lr: 1.0000e-04\n",
      "Epoch 35/500\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 0.1108 - accuracy: 0.9586 - val_loss: 0.2175 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 36/500\n",
      "340/340 [==============================] - 2s 6ms/step - loss: 0.1115 - accuracy: 0.9590 - val_loss: 0.2211 - val_accuracy: 0.9360 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train with modified parameters\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=500,            \n",
    "    batch_size=16,        \n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    class_weight=class_weight_dict  # Use the computed class weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.9590\n",
      "Final Validation Accuracy: 0.9360\n"
     ]
    }
   ],
   "source": [
    "# Print final metrics\n",
    "final_train_accuracy = history.history['accuracy'][-1]\n",
    "final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "print(f'Final Training Accuracy: {final_train_accuracy:.4f}')\n",
    "print(f'Final Validation Accuracy: {final_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('chatbot_model.h5')\n",
    "print('Training completed and model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
